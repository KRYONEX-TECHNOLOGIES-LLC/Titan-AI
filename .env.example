# Titan AI Environment Configuration
# Copy this file to .env.local and fill in your values

# ===================
# AI API Keys
# ===================

# Anthropic API Key (for Claude models)
ANTHROPIC_API_KEY=

# OpenAI API Key (for GPT models)
OPENAI_API_KEY=

# OpenRouter API Key (for model routing)
OPENROUTER_API_KEY=

# Voyage AI API Key (for embeddings)
VOYAGE_API_KEY=

# ===================
# Local AI (Optional)
# ===================

# Ollama Base URL
OLLAMA_BASE_URL=http://localhost:11434

# LiteLLM Proxy URL
LITELLM_PROXY_URL=http://localhost:4000

# ===================
# Feature Flags
# ===================

# Enable AI features
TITAN_AI_ENABLED=true

# Enable telemetry (disabled by default)
TITAN_TELEMETRY_ENABLED=false

# Enable experimental features
TITAN_EXPERIMENTAL=false

# ===================
# Performance
# ===================

# Maximum tokens for context
TITAN_MAX_CONTEXT_TOKENS=128000

# Enable GPU acceleration
TITAN_GPU_ENABLED=true

# Cache directory
TITAN_CACHE_DIR=.titan-cache

# ===================
# Development
# ===================

# Node environment
NODE_ENV=development

# Debug mode
DEBUG=titan:*
