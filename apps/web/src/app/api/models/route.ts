/**
 * Model Registry API
 * Provides 30+ model variants from LiteLLM/OpenRouter
 */

import { NextResponse } from 'next/server';

export interface ModelInfo {
  id: string;
  providerModelId: string;
  name: string;
  provider: string;
  tier: 'frontier' | 'standard' | 'economy' | 'local';
  contextWindow: number;
  maxOutputTokens: number;
  supportsThinking: boolean;
  supportsVision: boolean;
  supportsTools: boolean;
  costPer1MInput: number;
  costPer1MOutput: number;
  description: string;
}

// Real models with correct OpenRouter model IDs
export const MODEL_REGISTRY: ModelInfo[] = [
  // ═══ ANTHROPIC ═══
  {
    id: 'claude-sonnet-4',
    providerModelId: 'anthropic/claude-sonnet-4-20250514',
    name: 'Claude Sonnet 4',
    provider: 'Anthropic',
    tier: 'frontier',
    contextWindow: 200000,
    maxOutputTokens: 64000,
    supportsThinking: true,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 3,
    costPer1MOutput: 15,
    description: 'Latest Claude with best coding ability',
  },
  {
    id: 'claude-opus-4',
    providerModelId: 'anthropic/claude-opus-4-20250514',
    name: 'Claude Opus 4',
    provider: 'Anthropic',
    tier: 'frontier',
    contextWindow: 200000,
    maxOutputTokens: 32000,
    supportsThinking: true,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 15,
    costPer1MOutput: 75,
    description: 'Most capable Claude model',
  },
  {
    id: 'claude-3.5-sonnet',
    providerModelId: 'anthropic/claude-3.5-sonnet',
    name: 'Claude 3.5 Sonnet',
    provider: 'Anthropic',
    tier: 'standard',
    contextWindow: 200000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 3,
    costPer1MOutput: 15,
    description: 'Great balance of speed and capability',
  },
  {
    id: 'claude-3.5-haiku',
    providerModelId: 'anthropic/claude-3.5-haiku',
    name: 'Claude 3.5 Haiku',
    provider: 'Anthropic',
    tier: 'economy',
    contextWindow: 200000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 0.8,
    costPer1MOutput: 4,
    description: 'Fast and affordable',
  },
  {
    id: 'claude-3-opus',
    providerModelId: 'anthropic/claude-3-opus',
    name: 'Claude 3 Opus',
    provider: 'Anthropic',
    tier: 'frontier',
    contextWindow: 200000,
    maxOutputTokens: 4096,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 15,
    costPer1MOutput: 75,
    description: 'Previous flagship model',
  },

  // ═══ OPENAI ═══
  {
    id: 'gpt-4o',
    providerModelId: 'openai/gpt-4o',
    name: 'GPT-4o',
    provider: 'OpenAI',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 16384,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 2.5,
    costPer1MOutput: 10,
    description: 'Multimodal with fast responses',
  },
  {
    id: 'gpt-4o-mini',
    providerModelId: 'openai/gpt-4o-mini',
    name: 'GPT-4o Mini',
    provider: 'OpenAI',
    tier: 'economy',
    contextWindow: 128000,
    maxOutputTokens: 16384,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 0.15,
    costPer1MOutput: 0.6,
    description: 'Cost-efficient for simple tasks',
  },
  {
    id: 'gpt-4-turbo',
    providerModelId: 'openai/gpt-4-turbo',
    name: 'GPT-4 Turbo',
    provider: 'OpenAI',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 4096,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 10,
    costPer1MOutput: 30,
    description: 'Previous flagship with vision',
  },
  {
    id: 'o1',
    providerModelId: 'openai/o1',
    name: 'o1',
    provider: 'OpenAI',
    tier: 'frontier',
    contextWindow: 200000,
    maxOutputTokens: 100000,
    supportsThinking: true,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 15,
    costPer1MOutput: 60,
    description: 'Advanced reasoning capabilities',
  },
  {
    id: 'o1-mini',
    providerModelId: 'openai/o1-mini',
    name: 'o1 Mini',
    provider: 'OpenAI',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 65536,
    supportsThinking: true,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 3,
    costPer1MOutput: 12,
    description: 'Faster reasoning model',
  },
  {
    id: 'o3-mini',
    providerModelId: 'openai/o3-mini',
    name: 'o3 Mini',
    provider: 'OpenAI',
    tier: 'standard',
    contextWindow: 200000,
    maxOutputTokens: 100000,
    supportsThinking: true,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 1.1,
    costPer1MOutput: 4.4,
    description: 'Latest reasoning model',
  },

  // ═══ GOOGLE ═══
  {
    id: 'gemini-2.0-flash',
    providerModelId: 'google/gemini-2.0-flash-001',
    name: 'Gemini 2.0 Flash',
    provider: 'Google',
    tier: 'standard',
    contextWindow: 1000000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 0.1,
    costPer1MOutput: 0.4,
    description: 'Fast and multimodal',
  },
  {
    id: 'gemini-1.5-pro',
    providerModelId: 'google/gemini-pro-1.5',
    name: 'Gemini 1.5 Pro',
    provider: 'Google',
    tier: 'standard',
    contextWindow: 1000000,
    maxOutputTokens: 8192,
    supportsThinking: true,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 1.25,
    costPer1MOutput: 5,
    description: 'Long-context reasoning',
  },
  {
    id: 'gemini-1.5-flash',
    providerModelId: 'google/gemini-flash-1.5',
    name: 'Gemini 1.5 Flash',
    provider: 'Google',
    tier: 'economy',
    contextWindow: 1000000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 0.075,
    costPer1MOutput: 0.3,
    description: 'Fast and cost-effective',
  },

  // ═══ DEEPSEEK ═══
  {
    id: 'deepseek-chat',
    providerModelId: 'deepseek/deepseek-chat',
    name: 'DeepSeek Chat',
    provider: 'DeepSeek',
    tier: 'economy',
    contextWindow: 64000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.14,
    costPer1MOutput: 0.28,
    description: 'Excellent code generation',
  },
  {
    id: 'deepseek-r1',
    providerModelId: 'deepseek/deepseek-r1',
    name: 'DeepSeek R1',
    provider: 'DeepSeek',
    tier: 'standard',
    contextWindow: 64000,
    maxOutputTokens: 8192,
    supportsThinking: true,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.55,
    costPer1MOutput: 2.19,
    description: 'Reasoning with chain-of-thought',
  },

  // ═══ MISTRAL ═══
  {
    id: 'mistral-large',
    providerModelId: 'mistralai/mistral-large-2411',
    name: 'Mistral Large',
    provider: 'Mistral',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 2,
    costPer1MOutput: 6,
    description: 'European frontier model',
  },
  {
    id: 'mistral-small',
    providerModelId: 'mistralai/mistral-small-2503',
    name: 'Mistral Small',
    provider: 'Mistral',
    tier: 'economy',
    contextWindow: 32000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.1,
    costPer1MOutput: 0.3,
    description: 'Fast and efficient',
  },
  {
    id: 'codestral',
    providerModelId: 'mistralai/codestral-2501',
    name: 'Codestral',
    provider: 'Mistral',
    tier: 'standard',
    contextWindow: 256000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.3,
    costPer1MOutput: 0.9,
    description: 'Specialized for code',
  },

  // ═══ META ═══
  {
    id: 'llama-3.3-70b',
    providerModelId: 'meta-llama/llama-3.3-70b-instruct',
    name: 'Llama 3.3 70B',
    provider: 'Meta',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.35,
    costPer1MOutput: 0.4,
    description: 'Open-source powerhouse',
  },
  {
    id: 'llama-3.1-405b',
    providerModelId: 'meta-llama/llama-3.1-405b-instruct',
    name: 'Llama 3.1 405B',
    provider: 'Meta',
    tier: 'frontier',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 2.7,
    costPer1MOutput: 2.7,
    description: 'Largest open model',
  },

  // ═══ COHERE ═══
  {
    id: 'command-r-plus',
    providerModelId: 'cohere/command-r-plus-08-2024',
    name: 'Command R+',
    provider: 'Cohere',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 4096,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 2.5,
    costPer1MOutput: 10,
    description: 'RAG-optimized model',
  },
  {
    id: 'command-r',
    providerModelId: 'cohere/command-r-08-2024',
    name: 'Command R',
    provider: 'Cohere',
    tier: 'economy',
    contextWindow: 128000,
    maxOutputTokens: 4096,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.15,
    costPer1MOutput: 0.6,
    description: 'Efficient retrieval model',
  },

  // ═══ XAI ═══
  {
    id: 'grok-2',
    providerModelId: 'x-ai/grok-2-1212',
    name: 'Grok 2',
    provider: 'xAI',
    tier: 'standard',
    contextWindow: 131072,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: true,
    supportsTools: true,
    costPer1MInput: 2,
    costPer1MOutput: 10,
    description: 'Real-time knowledge access',
  },

  // ═══ QWEN ═══
  {
    id: 'qwen-2.5-72b',
    providerModelId: 'qwen/qwen-2.5-72b-instruct',
    name: 'Qwen 2.5 72B',
    provider: 'Qwen',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.35,
    costPer1MOutput: 0.4,
    description: 'Strong multilingual model',
  },
  {
    id: 'qwen-2.5-coder-32b',
    providerModelId: 'qwen/qwen-2.5-coder-32b-instruct',
    name: 'Qwen 2.5 Coder 32B',
    provider: 'Qwen',
    tier: 'standard',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0.18,
    costPer1MOutput: 0.18,
    description: 'Excellent for coding',
  },

  // ═══ LOCAL MODELS (Ollama) ═══
  {
    id: 'ollama-llama3.2',
    providerModelId: 'ollama/llama3.2',
    name: 'Llama 3.2 (Local)',
    provider: 'Local (Ollama)',
    tier: 'local',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0,
    costPer1MOutput: 0,
    description: 'Free local model',
  },
  {
    id: 'ollama-qwen2.5-coder',
    providerModelId: 'ollama/qwen2.5-coder',
    name: 'Qwen 2.5 Coder (Local)',
    provider: 'Local (Ollama)',
    tier: 'local',
    contextWindow: 32000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0,
    costPer1MOutput: 0,
    description: 'Free local code model',
  },
  {
    id: 'ollama-deepseek-coder',
    providerModelId: 'ollama/deepseek-coder-v2',
    name: 'DeepSeek Coder V2 (Local)',
    provider: 'Local (Ollama)',
    tier: 'local',
    contextWindow: 128000,
    maxOutputTokens: 8192,
    supportsThinking: false,
    supportsVision: false,
    supportsTools: true,
    costPer1MInput: 0,
    costPer1MOutput: 0,
    description: 'Fast local coding assistant',
  },
];

/**
 * GET /api/models - List all available models
 */
export async function GET() {
  // Group by provider
  const byProvider: Record<string, ModelInfo[]> = {};
  
  for (const model of MODEL_REGISTRY) {
    if (!byProvider[model.provider]) {
      byProvider[model.provider] = [];
    }
    byProvider[model.provider].push(model);
  }

  return NextResponse.json({
    models: MODEL_REGISTRY,
    byProvider,
    total: MODEL_REGISTRY.length,
    providers: Object.keys(byProvider),
  });
}
